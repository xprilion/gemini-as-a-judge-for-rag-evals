# Gemini as a Judge for RAG Evaluation

This repository demonstrates how to use powerful Large Language Models (LLMs) like Gemini to generate ground truth datasets for evaluating Retrieval-Augmented Generation (RAG) pipelines, with a focus on real-world data. The typical approach of manually creating QA pairs can be time-consuming and expensive. This repo showcases an alternative: leveraging the reasoning capabilities of Gemini to generate synthetic QA pairs from your existing data, providing a scalable and cost-effective solution for RAG evaluation.

## Project Structure

```
├── Step_1_Problem_Context_The_RAG.ipynb # Introduction to RAG and the problem
├── Step_2_Eval_Dataset.ipynb          # Notebook demonstrating ground truth dataset generation
├── Step_3_Perform_Eval.ipynb          # Notebook showcasing how to use the generated dataset to eval RAG
├── qna_dataset.json                 # Example of a generated question-answer dataset
└── reviews.json                     # Example data source (product reviews)
```

## Quick Start Guide

Here's a breakdown of the key steps involved:

1.  **Understand the RAG Context:** The `Step_1_Problem_Context_The_RAG.ipynb` notebook provides a general framework for building a RAG. It shows the loading of the product reviews dataset into Qdrant and how to fetch from it while using Gemini for producing answers to user's queries.

2.  **Generate the Evaluation Dataset:** The core logic resides in `Step_2_Eval_Dataset.ipynb`. This notebook takes your data (e.g., product reviews from `reviews.json`) and uses Gemini (or another powerful LLM) to:

    *   **Understand the Content:** The LLM processes each document (review) to grasp its meaning.
    *   **Generate Questions:** Based on the content, the LLM formulates relevant questions that can be answered using the information within the document.
    *   **Extract Answers:** The LLM identifies the correct answer to each generated question directly from the document.  It also captures the relevant index from where the answer was extracted.
    *   **Create QA Pairs:**  The questions and answers are paired to create a synthetic ground truth dataset, as exemplified by `qna_dataset.json`.  This dataset contains questions, their corresponding answers, and the indexes of the document segments where the answer can be found.

3.  **Perform RAG Evaluation:** The `Step_3_Perform_Eval.ipynb` notebook then uses the synthesized QA dataset to evaluate a RAG pipeline.  This involves:

    *   **Querying the RAG System:** For each question in the dataset, the RAG pipeline is queried.
    *   **Comparing Results:** The response from the RAG pipeline is compared to the ground truth answer generated by Gemini.
    *   **Calculating Metrics:**  Metrics like accuracy and latency are calculated to assess the RAG pipeline's performance. The notebook evaluates the RAG system's accuracy and latency. We further connect the evals with Weave to get proper insights into what is happening at each step.

## Key Takeaways

*   **LLMs as Data Generators:** This approach highlights the power of LLMs in generating high-quality, synthetic data for tasks like RAG evaluation.
*   **Scalable Evaluation:** Automating ground truth dataset creation enables you to scale your RAG evaluation efforts, especially when dealing with large volumes of data.
*   **Real-World Data Focus:** The use of realistic data (like product reviews) ensures that the evaluation is relevant to your specific use case.
*   **Cost-Effective:** Compared to manual annotation, using LLMs to create ground truth data can be significantly more cost-effective.

## Next Steps

To use this repository effectively:

1.  **Set up your environment:** Install the necessary libraries (specified in the notebooks).
2.  **Obtain a Gemini API Key:** You'll need an API key for Gemini (or another LLM provider) to run the dataset generation and evaluation notebooks.
3.  **Customize the Prompts:** Experiment with different prompts to the LLM to optimize the quality and relevance of the generated QA pairs.
4.  **Adapt to your Data:** Modify the data loading and processing steps to work with your specific data sources.
5.  **Explore Different RAG Architectures:** Use the generated dataset to evaluate different RAG pipeline configurations (e.g., different embedding models, retrieval strategies, and generation models).

